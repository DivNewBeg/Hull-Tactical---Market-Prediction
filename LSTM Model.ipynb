{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89879327-66e9-4d38-a73e-98ca8917bc7c",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "The evaluation API requires that you set up a server which will respond to inference requests.\n",
    "We have already defined the server; you just need write the predict function.\n",
    "When we evaluate your submission on the hidden test set the client defined in `default_gateway` will run in a different container\n",
    "with direct access to the hidden test set and hand off the data timestep by timestep.\n",
    "\n",
    "Your code will always have access to the published copies of the copmetition files.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545b5860-4b9c-45e3-abd8-ac2b66faebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "import importlib\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19614147-e0eb-46a7-827b-6dd25bbf135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x258e51b54f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd55b796-8126-4fe2-acd7-da69bb003296",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_id = 100\n",
    "last_seq_start_date_id = 71\n",
    "seq_start_date_id = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4693d234-d22a-4aa9-8750-fa4bb4e5c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence start date = 40, end date = 69\n",
      "Sequence start date = 41, end date = 70\n",
      "Sequence start date = 42, end date = 71\n",
      "Sequence start date = 43, end date = 72\n",
      "Sequence start date = 44, end date = 73\n",
      "Sequence start date = 45, end date = 74\n",
      "Sequence start date = 46, end date = 75\n",
      "Sequence start date = 47, end date = 76\n",
      "Sequence start date = 48, end date = 77\n",
      "Sequence start date = 49, end date = 78\n",
      "Sequence start date = 50, end date = 79\n",
      "Sequence start date = 51, end date = 80\n",
      "Sequence start date = 52, end date = 81\n",
      "Sequence start date = 53, end date = 82\n",
      "Sequence start date = 54, end date = 83\n",
      "Sequence start date = 55, end date = 84\n",
      "Sequence start date = 56, end date = 85\n",
      "Sequence start date = 57, end date = 86\n",
      "Sequence start date = 58, end date = 87\n",
      "Sequence start date = 59, end date = 88\n",
      "Sequence start date = 60, end date = 89\n",
      "Sequence start date = 61, end date = 90\n",
      "Sequence start date = 62, end date = 91\n",
      "Sequence start date = 63, end date = 92\n",
      "Sequence start date = 64, end date = 93\n",
      "Sequence start date = 65, end date = 94\n",
      "Sequence start date = 66, end date = 95\n",
      "Sequence start date = 67, end date = 96\n",
      "Sequence start date = 68, end date = 97\n",
      "Sequence start date = 69, end date = 98\n",
      "Sequence start date = 70, end date = 99\n",
      "Sequence start date = 71, end date = 100\n",
      "Number of sequences =  32\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "while seq_start_date_id <= last_seq_start_date_id:\n",
    "    \n",
    "    print(f\"Sequence start date = {seq_start_date_id}, end date = {seq_start_date_id + 29}\")\n",
    "    count += 1\n",
    "    seq_start_date_id += 1\n",
    "print(\"Number of sequences = \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53fade-1254-42ad-8fcc-83becbb20dd6",
   "metadata": {},
   "source": [
    "# Read the entire train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9662f12-1a05-4ccc-b648-a3953f742fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>forward_returns</th>\n",
       "      <th>risk_free_rate</th>\n",
       "      <th>market_forward_excess_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002421</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>-0.003038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008495</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>-0.009114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009624</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>-0.010243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_id  D1  D2  D3  D4  D5  D6  D7  D8  D9  ...  V3  V4  V5  V6  V7  V8  \\\n",
       "0        0   0   0   0   1   1   0   0   0   1  ... NaN NaN NaN NaN NaN NaN   \n",
       "1        1   0   0   0   1   1   0   0   0   1  ... NaN NaN NaN NaN NaN NaN   \n",
       "2        2   0   0   0   1   0   0   0   0   1  ... NaN NaN NaN NaN NaN NaN   \n",
       "\n",
       "   V9  forward_returns  risk_free_rate  market_forward_excess_returns  \n",
       "0 NaN        -0.002421        0.000301                      -0.003038  \n",
       "1 NaN        -0.008495        0.000303                      -0.009114  \n",
       "2 NaN        -0.009624        0.000301                      -0.010243  \n",
       "\n",
       "[3 rows x 98 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_dataset_df = pd.read_csv(\"./kaggle/input/hull-tactical-market-prediction/train complete.csv\")\n",
    "comp_dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c0583-08da-4f24-82ff-4dbe63618e8b",
   "metadata": {},
   "source": [
    "# Transformers for data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f3535-2c9b-4f2e-834b-fea3358c29bf",
   "metadata": {},
   "source": [
    "## Transformers for creating lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f80d328-2bb7-4eeb-9037-712ba3143185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_ind_list):\n",
    "        self.col_ind_list = col_ind_list\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.fitted = True\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "\n",
    "        X_shifted = np.empty_like(X, dtype=float)\n",
    "        \n",
    "        for col_ind in self.col_ind_list:\n",
    "\n",
    "            X_shifted[1:, col_ind] = X[:-1, col_ind]\n",
    "            X_shifted[0, col_ind] = np.nan\n",
    "            \"\"\"else:\n",
    "                print('here2')\n",
    "                X_shifted[:, col_ind] = X[:, col_ind]\"\"\"\n",
    "        \n",
    "        return X_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e72c2-c974-4fff-8aca-7b3e1d26c5ed",
   "metadata": {},
   "source": [
    "## Transformers for cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee04bce-d8cd-41d7-a45e-bd52c59c7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanerTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        self.col_mean_dict = {}\n",
    "\n",
    "        #For each column, determine the mean value.\n",
    "        for col_ind in range(X.shape[1]):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                col_mean = np.nanmean(X[:, col_ind])\n",
    "                \n",
    "            if np.isnan(col_mean):\n",
    "                col_mean = 0.0\n",
    "            self.col_mean_dict[col_ind] = col_mean\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "\n",
    "        X_copy = X.copy().astype(float)\n",
    "\n",
    "        for col_ind in range(X_copy.shape[1]):\n",
    "            nan_mask = np.isnan(X_copy[:, col_ind])\n",
    "            X_copy[nan_mask, col_ind] = self.col_mean_dict[col_ind]\n",
    "\n",
    "        \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6549de-7f40-4b54-8709-29a49cba4f29",
   "metadata": {},
   "source": [
    "## Transformer for creating latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c019709-34f5-4224-8820-856671292c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_sing_vals):\n",
    "        self.num_sing_vals = num_sing_vals\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        U, s, Vt = np.linalg.svd(X)\n",
    "        \n",
    "        self.U_ld = U[:, :self.num_sing_vals]\n",
    "        self.S_ld = s[0:self.num_sing_vals].reshape((1, self.num_sing_vals)) * np.eye(self.num_sing_vals)\n",
    "        self.Vt_ld = Vt[0:self.num_sing_vals, :]\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "\n",
    "        latent = np.dot(X, self.Vt_ld.T)\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e032633-b49a-414d-a92f-7c9a181a5541",
   "metadata": {},
   "source": [
    "# Create and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e1f7a5-f924-49f6-bd49-3e395630daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58783303-0d4c-4bca-ad44-79d206e9c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_score(forward_returns_ten: torch.Tensor, risk_free_rate_ten: torch.Tensor, signal_pred_ten: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates a smoothed version of the competition metric.\n",
    "\n",
    "    This metric penalizes strategies that take on significantly more volatility than the underlying market.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clip positions to allowed range\n",
    "    signal_pred = torch.clamp(signal_pred_ten, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "    #Strategy returns\n",
    "    strategy_returns = risk_free_rate_ten * (1 - signal_pred) + signal_pred * forward_returns_ten\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    strategy_excess_returns = strategy_returns - risk_free_rate_ten\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1.0 / forward_returns_ten.shape[0]) - 1\n",
    "    strategy_std = torch.std(strategy_returns, unbiased = False)\n",
    "\n",
    "    trading_days_per_yr = torch.tensor(252, dtype=torch.float32)\n",
    "    if strategy_std == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, strategy std is zero')\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * torch.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = strategy_std * torch.sqrt(trading_days_per_yr) * 100.0\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    market_excess_returns = forward_returns_ten - risk_free_rate_ten\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / forward_returns_ten.shape[0]) - 1\n",
    "    market_std = torch.std(forward_returns_ten, unbiased = False)\n",
    "    \n",
    "    market_volatility = market_std * torch.sqrt(trading_days_per_yr) * torch.tensor(100.0, dtype=torch.float32)\n",
    "\n",
    "    if market_volatility == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, market std is zero')\n",
    "\n",
    "    # Calculate the volatility penalty\n",
    "    excess_vol = torch.relu(strategy_volatility / market_volatility - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    # Calculate the return penalty\n",
    "    return_gap = torch.relu((market_mean_excess_return - strategy_mean_excess_return) * torch.tensor(100.0, dtype=torch.float32) * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / torch.tensor(100.0, dtype=torch.float32)\n",
    "\n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    adjusted_sharpe = torch.clamp(adjusted_sharpe, max = 1_000_000)\n",
    "    \n",
    "    return adjusted_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c737c01-742a-42e5-9219-138ff64508a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n",
    "\n",
    "    This metric penalizes strategies that take on significantly more volatility\n",
    "    than the underlying market.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission['prediction']):\n",
    "        raise ParticipantVisibleError('Predictions must be numeric')\n",
    "\n",
    "    solution = solution\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    if solution['position'].max() > MAX_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n",
    "    if solution['position'].min() < MIN_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n",
    "\n",
    "    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "\n",
    "    trading_days_per_yr = 252\n",
    "    if strategy_std == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, strategy std is zero')\n",
    "    \n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    \n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n",
    "    market_std = solution['forward_returns'].std()\n",
    "\n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n",
    "\n",
    "    if market_volatility == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, market std is zero')\n",
    "\n",
    "    # Calculate the volatility penalty\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    # Calculate the return penalty\n",
    "    return_gap = max(\n",
    "        0,\n",
    "        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n",
    "    )\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    \n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return min(float(adjusted_sharpe), 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f631e426-f95f-445e-bd58-27244d6be935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data_ten, forward_rets_ten, risk_free_rate_ten, seq_len, stride = 1):\n",
    "        self.data_ten = data_ten\n",
    "        self.forward_rets_ten = forward_rets_ten\n",
    "        self.risk_free_rate_ten = risk_free_rate_ten\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride\n",
    "        self.num_samples = (data_ten.shape[0] - seq_len)//self.stride + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.stride\n",
    "        x = self.data_ten[start : start + self.seq_len]  # shape (SEQ_LEN, INPUT_DIM)\n",
    "        fr = self.forward_rets_ten[start:start + self.seq_len]\n",
    "        rf = self.risk_free_rate_ten[start:start + self.seq_len]\n",
    "        return x, fr, rf, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058bb21b-b6cc-4df4-a7cd-27c9599eab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        out, (h_n, c_n) = self.lstm(x)  # out: (batch, seq_len, hidden_dim)\n",
    "        out = out[:, -1, :]              # take last timestep\n",
    "        out = self.fc(out)               # shape: (batch, output_dim)\n",
    "        out = torch.sigmoid(out) * 2     # enforce [0, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e7bd55-c7d6-40ea-97e8-dbc585570518",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_model = None\n",
    "\n",
    "def train_model(lt_train_ten, forward_rets_ten, risk_free_rate_ten, lt_val_ten, forward_rets_val_ten, risk_free_rate_val_ten, lt_seq_len, lt_input_dim, lt_hidden_dim, output_dim, shuffle, num_epochs = None, patience = 2, verbose = False):\n",
    "\n",
    "    global lt_model\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if lt_val_ten is not None:\n",
    "        BATCH_SIZE = lt_val_ten.shape[0]\n",
    "    else:\n",
    "        BATCH_SIZE = 8\n",
    "\n",
    "    stride = 1\n",
    "\n",
    "    lt_dataset = SequenceDataset(lt_train_ten, forward_rets_ten, risk_free_rate_ten, lt_seq_len, stride = stride)\n",
    "    lt_dataloader = DataLoader(lt_dataset, batch_size=BATCH_SIZE, shuffle=shuffle)\n",
    "    \n",
    "    lt_model = SimpleLSTM(input_dim=lt_input_dim, hidden_dim=lt_hidden_dim, output_dim=output_dim)\n",
    "    \n",
    "    total_count = 0\n",
    "    for name, param in lt_model.named_parameters():\n",
    "            \n",
    "        count = param.numel()\n",
    "        total_count += count\n",
    "    \n",
    "    #Validation metrics\n",
    "    if lt_val_ten is not None:\n",
    "        lt_val_dataset = SequenceDataset(lt_val_ten, forward_rets_val_ten, risk_free_rate_val_ten, lt_seq_len, stride = stride)\n",
    "        lt_val_dataloader = DataLoader(lt_val_dataset, batch_size=lt_val_ten.shape[0], shuffle=False)\n",
    "        val_batch, val_batch_fr_ten, val_batch_rf_ten, val_idx_ten = next(iter(lt_val_dataloader))\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(lt_model.parameters()), lr=0.003) \n",
    "    \n",
    "    total_loss = 0\n",
    "    total_adj_sharpe = 0\n",
    "    total_val_adj_sharpe = 0\n",
    "    \n",
    "    NUM_EPOCHS = 50\n",
    "\n",
    "    early_stop_count = 0\n",
    "    best_val_adj_sharpe = -np.inf\n",
    "    adj_sharpe_best_epoch = -np.inf\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        if num_epochs is not None:\n",
    "            if epoch >= num_epochs:\n",
    "                break\n",
    "        \n",
    "        total_abs = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        epoch_adj_sharpe = 0.0\n",
    "    \n",
    "        all_preds = []\n",
    "\n",
    "        batch_count = 0\n",
    "        for batch, batch_fr_ten, batch_rf_ten, idx_ten in lt_dataloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Signal predictions using the long-term model.\n",
    "            lt_preds_ten = lt_model(batch)\n",
    "            final_preds_ten = torch.sigmoid(lt_preds_ten) * 2\n",
    "            \n",
    "            #Compute loss as negative smoothed_score (maximize Sharpe)\n",
    "            batch_loss = -smoothed_score(\n",
    "                forward_returns_ten=batch_fr_ten,\n",
    "                risk_free_rate_ten=batch_rf_ten,\n",
    "                signal_pred_ten=final_preds_ten\n",
    "            )\n",
    "            \n",
    "            submission = pd.DataFrame()\n",
    "            submission['prediction'] = final_preds_ten.detach().numpy().flatten()\n",
    "            \n",
    "            solution = pd.DataFrame()\n",
    "            solution['forward_returns'] = batch_fr_ten.detach().numpy().flatten()\n",
    "            solution['risk_free_rate'] = batch_rf_ten.detach().numpy().flatten()\n",
    "            \n",
    "            batch_adj_sharpe = round(score(solution, submission, None), 3)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_adj_sharpe += batch_adj_sharpe\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            batch_count += 1\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        mean_loss_for_epoch = round(epoch_loss/batch_count, 3)\n",
    "        mean_adj_sharpe_for_epoch = round(epoch_adj_sharpe/batch_count, 3)\n",
    "\n",
    "        if lt_val_ten is not None:\n",
    "\n",
    "            #Signal predictions on the validation data using the long-term model.\n",
    "            lt_val_preds_ten = lt_model(val_batch)\n",
    "            \n",
    "            final_val_preds_ten = torch.sigmoid(lt_val_preds_ten) * 2\n",
    "            \n",
    "            val_submission = pd.DataFrame()\n",
    "            val_submission['prediction'] = final_val_preds_ten.detach().numpy().flatten()\n",
    "            val_solution = pd.DataFrame()\n",
    "            val_solution['forward_returns'] = val_batch_fr_ten.detach().numpy().flatten()\n",
    "            val_solution['risk_free_rate'] = val_batch_rf_ten.detach().numpy().flatten()\n",
    "            \n",
    "            val_adj_sharpe = round(score(val_solution, val_submission, None), 3)\n",
    "    \n",
    "            #Update the best validation score so far.\n",
    "            if best_val_adj_sharpe < val_adj_sharpe:\n",
    "                best_val_adj_sharpe = val_adj_sharpe\n",
    "                adj_sharpe_best_epoch = mean_adj_sharpe_for_epoch\n",
    "                early_stop_count = 0\n",
    "                best_epoch = epoch\n",
    "    \n",
    "            else:\n",
    "                if early_stop_count > patience:\n",
    "                    break\n",
    "    \n",
    "                else:\n",
    "                    early_stop_count += 1\n",
    "            \n",
    "            if verbose == True:\n",
    "                print(f\"Epoch {epoch}: Mean Training Loss = {mean_loss_for_epoch}, Training mean adj sharpe ratio = {mean_adj_sharpe_for_epoch}, Validation mean adj sharpe ratio = {val_adj_sharpe}\")\n",
    "\n",
    "    time_taken = round((time.time() - start_time)/60, 3)\n",
    "    if verbose == True:\n",
    "        print(f\"Best epoch: {best_epoch}\")\n",
    "        print(f\"Best mean adjusted sharpe ratio (Val): {best_val_adj_sharpe}\")\n",
    "        print(f\"Time taken = {time_taken} minutes\")\n",
    "\n",
    "    return lt_model, total_count, adj_sharpe_best_epoch, best_val_adj_sharpe, time_taken, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52851e23-d18e-4f14-b0c7-514b4006c432",
   "metadata": {},
   "source": [
    "## Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885f4747-2df7-491f-8fe8-74670d6ef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(col_ser):\n",
    "    \n",
    "    global col_number\n",
    "    min_val = col_ser.min()\n",
    "    max_val = col_ser.max()\n",
    "    \n",
    "    return (col_ser - min_val)/(max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d706e6b4-3ffb-4168-92a3-c4e3caaa7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_ds_scaled_df = comp_dataset_df.apply(scale, axis = 0)\n",
    "comp_ds_scaled_df['date_id'] = comp_dataset_df['date_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d0b312c-c488-4fc0-b343-d43496c9d3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>forward_returns</th>\n",
       "      <th>risk_free_rate</th>\n",
       "      <th>market_forward_excess_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.464247</td>\n",
       "      <td>0.949351</td>\n",
       "      <td>0.462739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388720</td>\n",
       "      <td>0.955528</td>\n",
       "      <td>0.387855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374671</td>\n",
       "      <td>0.950587</td>\n",
       "      <td>0.373946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_id   D1   D2   D3   D4   D5   D6   D7   D8   D9  ...  V3  V4  V5  V6  \\\n",
       "0        0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0  ... NaN NaN NaN NaN   \n",
       "1        1  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0  ... NaN NaN NaN NaN   \n",
       "2        2  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  1.0  ... NaN NaN NaN NaN   \n",
       "\n",
       "   V7  V8  V9  forward_returns  risk_free_rate  market_forward_excess_returns  \n",
       "0 NaN NaN NaN         0.464247        0.949351                       0.462739  \n",
       "1 NaN NaN NaN         0.388720        0.955528                       0.387855  \n",
       "2 NaN NaN NaN         0.374671        0.950587                       0.373946  \n",
       "\n",
       "[3 rows x 98 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ds_scaled_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b570e0-c0a0-497a-8327-2a95d7a44dc9",
   "metadata": {},
   "source": [
    "# Run TSCV to determine the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d97e75e-1c14-460b-9a6e-48e5950250f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tscv(train_size, val_size, seq_len, step_size, input_dim, hidden_dim, comp_dataset_df, shuffle, max_folds = None, verbose = False):\n",
    "\n",
    "    root = './kaggle/input/hull-tactical-market-prediction'\n",
    "    if os.path.exists(root):\n",
    "        for item in os.listdir(root):\n",
    "            item_path = os.path.join(root, item)\n",
    "            if os.path.isdir(item_path):   # delete only folders\n",
    "                shutil.rmtree(item_path)    \n",
    "    \n",
    "    #Initialising the validation indices.\n",
    "    val_start_index = comp_dataset_df.shape[0] - val_size\n",
    "    val_end_index = comp_dataset_df.shape[0] - 1\n",
    "\n",
    "    #Initialise the train_set1 indices\n",
    "    train_set_end_index = val_start_index - 1\n",
    "    train_set_start_index = train_set_end_index - train_size + 1\n",
    "\n",
    "    #Now do the actual cross validation.\n",
    "    count = 0\n",
    "    \n",
    "    fold_no = 0\n",
    "    \n",
    "    mean_adj_sharpe_across_fold = 0\n",
    "    mean_val_adj_sharpe_across_fold = 0\n",
    "    mean_time_taken_across_fold = 0\n",
    "    mean_num_epochs = 0\n",
    "    \n",
    "    #Create as many folds as possible.\n",
    "    while train_set_start_index > 0:\n",
    "\n",
    "        if max_folds is not None:\n",
    "            if fold_no >= max_folds:\n",
    "                break\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f'Training test fold: {fold_no}')\n",
    "        \n",
    "        comp_dataset_df['lagged_forward_returns'] = comp_dataset_df['forward_returns'].shift(1)\n",
    "        comp_dataset_df['lagged_risk_free_rate'] = comp_dataset_df['risk_free_rate'].shift(1)\n",
    "        comp_dataset_df['lagged_market_forward_excess_returns'] = comp_dataset_df['market_forward_excess_returns'].shift(1)\n",
    "    \n",
    "        train_set_df = comp_dataset_df.iloc[train_set_start_index:train_set_end_index+1, :]\n",
    "        val_set_df = comp_dataset_df.iloc[val_start_index:val_end_index+1, :]\n",
    "        \n",
    "        folder = f'./kaggle/input/hull-tactical-market-prediction/Fold {fold_no}'\n",
    "        \n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        train_set_df = train_set_df.drop(['lagged_forward_returns', 'lagged_risk_free_rate', 'lagged_market_forward_excess_returns'], axis = 1)\n",
    "        train_set_df.to_csv(f'{folder}/train_set.csv', index = False)\n",
    "\n",
    "        val_set_subset_df = val_set_df[['forward_returns', 'risk_free_rate', 'market_forward_excess_returns']].copy()\n",
    "        val_set_df = val_set_df.drop(['forward_returns', 'risk_free_rate', 'market_forward_excess_returns'], axis = 1)\n",
    "        val_set_df.to_csv(f'{folder}/test.csv', index = False)\n",
    "    \n",
    "        pipeline = Pipeline([('cleaner', CleanerTransformer()),\n",
    "                             ('latent', LatentTransformer(input_dim))])\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                category=FutureWarning,\n",
    "                module=\"sklearn.pipeline\"\n",
    "            )\n",
    "            pipeline = pipeline.fit(train_set_df.values)\n",
    "            lt_train_latent_ten = torch.tensor(pipeline.transform(train_set_df.values), dtype=torch.float32)\n",
    "            lt_val_latent_ten = torch.tensor(pipeline.transform(val_set_df.values), dtype=torch.float32)\n",
    "\n",
    "            #Replace infinite values.\n",
    "            if torch.isinf(lt_val_latent_ten).any():\n",
    "                #print('$%$#%#$%#$%#$%#$%Infs in validation data.#$%#$%#$%#$%#$%#$')\n",
    "                finite_vals = lt_val_latent_ten[torch.isfinite(lt_val_latent_ten)]\n",
    "                max_val = finite_vals.max()\n",
    "                min_val = finite_vals.min()\n",
    "                lt_val_latent_ten[lt_val_latent_ten == float('inf')] = max_val\n",
    "                lt_val_latent_ten[lt_val_latent_ten == float('-inf')] = min_val\n",
    "        \n",
    "        #We will then train the model here.\n",
    "        forward_returns_ten = torch.tensor(train_set_df['forward_returns'].values, dtype=torch.float32)\n",
    "        risk_free_rate_ten = torch.tensor(train_set_df['risk_free_rate'].values, dtype=torch.float32)\n",
    "        forward_rets_val_ten = torch.tensor(val_set_subset_df['forward_returns'].values, dtype=torch.float32)\n",
    "        risk_free_rate_val_ten = torch.tensor(val_set_subset_df['risk_free_rate'].values, dtype=torch.float32)\n",
    "\n",
    "        lt_model, param_count, adj_sharpe_best_epoch, best_val_adj_sharpe, time_taken, num_epochs = train_model(lt_train_latent_ten, forward_returns_ten, risk_free_rate_ten, lt_val_latent_ten, forward_rets_val_ten, risk_free_rate_val_ten, seq_len, input_dim, hidden_dim, 1, shuffle, num_epochs = 20, patience = 2, verbose = verbose)\n",
    "        \n",
    "        mean_adj_sharpe_across_fold += adj_sharpe_best_epoch\n",
    "        mean_val_adj_sharpe_across_fold += best_val_adj_sharpe\n",
    "        mean_time_taken_across_fold += time_taken\n",
    "        mean_num_epochs += num_epochs\n",
    "        \n",
    "        train_set_start_index = train_set_start_index - step_size\n",
    "        train_set_end_index = train_set_end_index - step_size\n",
    "        \n",
    "        val_start_index = val_start_index - step_size\n",
    "        val_end_index = val_end_index - step_size\n",
    "        \n",
    "        fold_no += 1\n",
    "\n",
    "        if verbose == True:\n",
    "            print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    mean_adj_sharpe_across_fold = round(mean_adj_sharpe_across_fold/fold_no, 3)\n",
    "    mean_val_adj_sharpe_across_fold = round(mean_val_adj_sharpe_across_fold/fold_no, 3)\n",
    "    mean_time_taken_across_fold = round(mean_time_taken_across_fold/fold_no, 3)\n",
    "    mean_num_epochs = int(round(mean_num_epochs/fold_no, 0))\n",
    "\n",
    "    print(f'Mean adjusted sharpe ratio across folds = {mean_adj_sharpe_across_fold}')\n",
    "    print(f'Mean adjusted sharpe ratio (Validation) across folds = {mean_val_adj_sharpe_across_fold}')\n",
    "    print(f'Mean time taken across folds = {mean_time_taken_across_fold}')\n",
    "    print(f'Mean number of epochs across folds = {mean_num_epochs}')\n",
    "    \n",
    "    return param_count, mean_adj_sharpe_across_fold, mean_val_adj_sharpe_across_fold, mean_time_taken_across_fold, mean_num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d83641b5-dce1-489a-9426-8bebd368a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.483\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.36\n",
      "Mean time taken across folds = 0.094\n",
      "Mean number of epochs across folds = 10\n",
      "Validation size = 8, Train size = 504, Hidden dim = 4, Input dimension: 4, Parameter count = 165, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.486\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.388\n",
      "Mean time taken across folds = 0.089\n",
      "Mean number of epochs across folds = 11\n",
      "Validation size = 8, Train size = 504, Hidden dim = 4, Input dimension: 8, Parameter count = 229, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.492\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.39\n",
      "Mean time taken across folds = 0.117\n",
      "Mean number of epochs across folds = 10\n",
      "Validation size = 8, Train size = 504, Hidden dim = 4, Input dimension: 16, Parameter count = 357, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.479\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.414\n",
      "Mean time taken across folds = 0.085\n",
      "Mean number of epochs across folds = 9\n",
      "Validation size = 8, Train size = 504, Hidden dim = 8, Input dimension: 4, Parameter count = 457, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.473\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.443\n",
      "Mean time taken across folds = 0.087\n",
      "Mean number of epochs across folds = 9\n",
      "Validation size = 8, Train size = 504, Hidden dim = 8, Input dimension: 8, Parameter count = 585, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.484\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.43\n",
      "Mean time taken across folds = 0.081\n",
      "Mean number of epochs across folds = 9\n",
      "Validation size = 8, Train size = 504, Hidden dim = 8, Input dimension: 16, Parameter count = 841, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.486\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.418\n",
      "Mean time taken across folds = 0.069\n",
      "Mean number of epochs across folds = 6\n",
      "Validation size = 8, Train size = 504, Hidden dim = 16, Input dimension: 4, Parameter count = 1425, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.488\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.415\n",
      "Mean time taken across folds = 0.071\n",
      "Mean number of epochs across folds = 7\n",
      "Validation size = 8, Train size = 504, Hidden dim = 16, Input dimension: 8, Parameter count = 1681, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.497\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.397\n",
      "Mean time taken across folds = 0.077\n",
      "Mean number of epochs across folds = 7\n",
      "Validation size = 8, Train size = 504, Hidden dim = 16, Input dimension: 16, Parameter count = 2193, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.507\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.413\n",
      "Mean time taken across folds = 0.165\n",
      "Mean number of epochs across folds = 7\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 4, Input dimension: 4, Parameter count = 165, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.51\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.426\n",
      "Mean time taken across folds = 0.165\n",
      "Mean number of epochs across folds = 7\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 4, Input dimension: 8, Parameter count = 229, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.509\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.396\n",
      "Mean time taken across folds = 0.167\n",
      "Mean number of epochs across folds = 6\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 4, Input dimension: 16, Parameter count = 357, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.511\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.396\n",
      "Mean time taken across folds = 0.145\n",
      "Mean number of epochs across folds = 6\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 8, Input dimension: 4, Parameter count = 457, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.524\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.378\n",
      "Mean time taken across folds = 0.15\n",
      "Mean number of epochs across folds = 5\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 8, Input dimension: 8, Parameter count = 585, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.512\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.398\n",
      "Mean time taken across folds = 0.147\n",
      "Mean number of epochs across folds = 6\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 8, Input dimension: 16, Parameter count = 841, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.526\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.326\n",
      "Mean time taken across folds = 0.117\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 16, Input dimension: 4, Parameter count = 1425, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.519\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.339\n",
      "Mean time taken across folds = 0.128\n",
      "Mean number of epochs across folds = 4\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 16, Input dimension: 8, Parameter count = 1681, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.53\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.343\n",
      "Mean time taken across folds = 0.116\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 1008, Hidden dim = 16, Input dimension: 16, Parameter count = 2193, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.818\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.398\n",
      "Mean time taken across folds = 0.18\n",
      "Mean number of epochs across folds = 4\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 4, Input dimension: 4, Parameter count = 165, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.813\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.381\n",
      "Mean time taken across folds = 0.185\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 4, Input dimension: 8, Parameter count = 229, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.816\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.396\n",
      "Mean time taken across folds = 0.199\n",
      "Mean number of epochs across folds = 4\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 4, Input dimension: 16, Parameter count = 357, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.804\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.401\n",
      "Mean time taken across folds = 0.172\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 8, Input dimension: 4, Parameter count = 457, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.812\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.368\n",
      "Mean time taken across folds = 0.182\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 8, Input dimension: 8, Parameter count = 585, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.807\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.395\n",
      "Mean time taken across folds = 0.176\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 8, Input dimension: 16, Parameter count = 841, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.834\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.315\n",
      "Mean time taken across folds = 0.15\n",
      "Mean number of epochs across folds = 2\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 16, Input dimension: 4, Parameter count = 1425, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.832\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.323\n",
      "Mean time taken across folds = 0.15\n",
      "Mean number of epochs across folds = 2\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 16, Input dimension: 8, Parameter count = 1681, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.838\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.288\n",
      "Mean time taken across folds = 0.15\n",
      "Mean number of epochs across folds = 2\n",
      "Validation size = 8, Train size = 1512, Hidden dim = 16, Input dimension: 16, Parameter count = 2193, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.91\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.407\n",
      "Mean time taken across folds = 0.383\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 2016, Hidden dim = 4, Input dimension: 4, Parameter count = 165, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.925\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.365\n",
      "Mean time taken across folds = 0.301\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 2016, Hidden dim = 4, Input dimension: 8, Parameter count = 229, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.914\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.388\n",
      "Mean time taken across folds = 0.237\n",
      "Mean number of epochs across folds = 3\n",
      "Validation size = 8, Train size = 2016, Hidden dim = 4, Input dimension: 16, Parameter count = 357, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n",
      "Mean adjusted sharpe ratio across folds = 2.916\n",
      "Mean adjusted sharpe ratio (Validation) across folds = 3.356\n",
      "Mean time taken across folds = 0.216\n",
      "Mean number of epochs across folds = 2\n",
      "Validation size = 8, Train size = 2016, Hidden dim = 8, Input dimension: 4, Parameter count = 457, Seq length = 1\n",
      "\n",
      "Maximum training/test folds = 50\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m     handles.handle,\n\u001b[32m    262\u001b[39m     lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m     quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[39m, in \u001b[36mCSVFormatter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_header()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28mself\u001b[39m._save_body()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[39m, in \u001b[36mCSVFormatter._save_body\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28mself\u001b[39m._save_chunk(start_i, end_i)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[39m, in \u001b[36mCSVFormatter._save_chunk\u001b[39m\u001b[34m(self, start_i, end_i)\u001b[39m\n\u001b[32m    323\u001b[39m ix = \u001b[38;5;28mself\u001b[39m.data_index[slicer]._get_values_for_csv(**\u001b[38;5;28mself\u001b[39m._number_format)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m libwriters.write_csv_rows(\n\u001b[32m    325\u001b[39m     data,\n\u001b[32m    326\u001b[39m     ix,\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m.nlevels,\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mself\u001b[39m.cols,\n\u001b[32m    329\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer,\n\u001b[32m    330\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mwriters.pyx:73\u001b[39m, in \u001b[36mpandas._libs.writers.write_csv_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m max_folds = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[32m5\u001b[39m * \u001b[32m576\u001b[39m/val_size, \u001b[32m0\u001b[39m), \u001b[32m50\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMaximum training/test folds = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m param_count, mean_adj_sharpe_across_fold, mean_val_adj_sharpe_across_fold, mean_time_taken_across_fold, num_epochs = run_tscv(train_size, val_size, seq_len, step_size, input_dim, hidden_dim, comp_dataset_df, max_folds = max_folds, shuffle = \u001b[38;5;28;01mFalse\u001b[39;00m, verbose = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mValidation size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Hidden dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Input dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Parameter count = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Seq length = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((best_mean_adj_sharpe_across_fold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (best_mean_val_adj_sharpe_across_fold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_tscv\u001b[39m\u001b[34m(train_size, val_size, seq_len, step_size, input_dim, hidden_dim, comp_dataset_df, shuffle, max_folds, verbose)\u001b[39m\n\u001b[32m     48\u001b[39m     os.makedirs(folder)\n\u001b[32m     50\u001b[39m train_set_df = train_set_df.drop([\u001b[33m'\u001b[39m\u001b[33mlagged_forward_returns\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlagged_risk_free_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlagged_market_forward_excess_returns\u001b[39m\u001b[33m'\u001b[39m], axis = \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m train_set_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/train_set.csv\u001b[39m\u001b[33m'\u001b[39m, index = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     53\u001b[39m val_set_subset_df = val_set_df[[\u001b[33m'\u001b[39m\u001b[33mforward_returns\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrisk_free_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmarket_forward_excess_returns\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m     54\u001b[39m val_set_df = val_set_df.drop([\u001b[33m'\u001b[39m\u001b[33mforward_returns\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrisk_free_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmarket_forward_excess_returns\u001b[39m\u001b[33m'\u001b[39m], axis = \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter).to_csv(\n\u001b[32m   3968\u001b[39m     path_or_buf,\n\u001b[32m   3969\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   3970\u001b[39m     sep=sep,\n\u001b[32m   3971\u001b[39m     encoding=encoding,\n\u001b[32m   3972\u001b[39m     errors=errors,\n\u001b[32m   3973\u001b[39m     compression=compression,\n\u001b[32m   3974\u001b[39m     quoting=quoting,\n\u001b[32m   3975\u001b[39m     columns=columns,\n\u001b[32m   3976\u001b[39m     index_label=index_label,\n\u001b[32m   3977\u001b[39m     mode=mode,\n\u001b[32m   3978\u001b[39m     chunksize=chunksize,\n\u001b[32m   3979\u001b[39m     quotechar=quotechar,\n\u001b[32m   3980\u001b[39m     date_format=date_format,\n\u001b[32m   3981\u001b[39m     doublequote=doublequote,\n\u001b[32m   3982\u001b[39m     escapechar=escapechar,\n\u001b[32m   3983\u001b[39m     storage_options=storage_options,\n\u001b[32m   3984\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m csv_formatter.save()\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m    254\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.encoding,\n\u001b[32m    255\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.errors,\n\u001b[32m    256\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.compression,\n\u001b[32m    257\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.storage_options,\n\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\common.py:157\u001b[39m, in \u001b[36mIOHandles.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    153\u001b[39m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    154\u001b[39m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    155\u001b[39m     traceback: TracebackType | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    156\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\AdditionalLibraries\\Lib\\site-packages\\pandas\\io\\common.py:144\u001b[39m, in \u001b[36mIOHandles.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mself\u001b[39m.created_handles.remove(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.created_handles:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     handle.close()\n\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m.created_handles = []\n\u001b[32m    146\u001b[39m \u001b[38;5;28mself\u001b[39m.is_wrapped = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE_LIST = [252*2, 252*4, 252*6, 252*8, 252*10] #[252*10, 252*15, 252*20, 252*25]\n",
    "VAL_SIZE_LIST = [8, 16, 32]\n",
    "\n",
    "#Model hyper-parameters.\n",
    "SEQ_LEN_LIST = [1]\n",
    "INPUT_DIM_LIST = [4, 8, 16]\n",
    "HIDDEN_DIM_LIST = [4, 8, 16]\n",
    "\n",
    "best_mean_adj_sharpe_across_fold = None\n",
    "best_mean_val_adj_sharpe_across_fold = None\n",
    "best_train_size = None\n",
    "best_seq_len = None\n",
    "best_input_dim = None\n",
    "best_hidden_dim = None\n",
    "best_param_count = None\n",
    "best_val_size = None\n",
    "best_mean_time = None\n",
    "best_num_epochs = None\n",
    "\n",
    "for val_size in VAL_SIZE_LIST:\n",
    "\n",
    "    step_size = val_size\n",
    "    \n",
    "    for train_size in TRAIN_SIZE_LIST:\n",
    "    \n",
    "        for hidden_dim in HIDDEN_DIM_LIST:\n",
    "            \n",
    "            for input_dim in INPUT_DIM_LIST:\n",
    "                \n",
    "                for seq_len in SEQ_LEN_LIST:\n",
    "\n",
    "                    max_folds = min(round(5 * 576/val_size, 0), 50)\n",
    "                    print(f'Maximum training/test folds = {max_folds}')\n",
    "            \n",
    "                    param_count, mean_adj_sharpe_across_fold, mean_val_adj_sharpe_across_fold, mean_time_taken_across_fold, num_epochs = run_tscv(train_size, val_size, seq_len, step_size, input_dim, hidden_dim, comp_dataset_df, max_folds = max_folds, shuffle = False, verbose = False)\n",
    "                    \n",
    "                    print(f'Validation size = {val_size}, Train size = {train_size}, Hidden dim = {hidden_dim}, Input dimension: {input_dim}, Parameter count = {param_count}, Seq length = {seq_len}\\n')\n",
    "                    if ((best_mean_adj_sharpe_across_fold is None) or (best_mean_val_adj_sharpe_across_fold is None)):\n",
    "                        best_mean_adj_sharpe_across_fold = mean_adj_sharpe_across_fold\n",
    "                        best_mean_val_adj_sharpe_across_fold = mean_val_adj_sharpe_across_fold\n",
    "                        best_train_size = train_size\n",
    "                        best_seq_len = seq_len\n",
    "                        best_input_dim = input_dim\n",
    "                        best_hidden_dim = hidden_dim\n",
    "                        best_param_count = param_count\n",
    "                        best_val_size = val_size\n",
    "                        best_mean_time = mean_time_taken_across_fold\n",
    "                        best_num_epochs = num_epochs\n",
    "    \n",
    "                    elif ((best_mean_val_adj_sharpe_across_fold == mean_val_adj_sharpe_across_fold) & (best_param_count > param_count)):\n",
    "                        best_mean_adj_sharpe_across_fold = mean_adj_sharpe_across_fold\n",
    "                        best_mean_val_adj_sharpe_across_fold = mean_val_adj_sharpe_across_fold\n",
    "                        best_train_size = train_size\n",
    "                        best_seq_len = seq_len\n",
    "                        best_input_dim = input_dim\n",
    "                        best_hidden_dim = hidden_dim\n",
    "                        best_param_count = param_count\n",
    "                        best_val_size = val_size\n",
    "                        best_mean_time = mean_time_taken_across_fold\n",
    "                        best_num_epochs = num_epochs\n",
    "                        \n",
    "                    elif best_mean_val_adj_sharpe_across_fold < mean_val_adj_sharpe_across_fold:\n",
    "                        best_mean_adj_sharpe_across_fold = mean_adj_sharpe_across_fold\n",
    "                        best_mean_val_adj_sharpe_across_fold = mean_val_adj_sharpe_across_fold\n",
    "                        best_train_size = train_size\n",
    "                        best_seq_len = seq_len\n",
    "                        best_input_dim = input_dim\n",
    "                        best_hidden_dim = hidden_dim\n",
    "                        best_param_count = param_count\n",
    "                        best_val_size = val_size\n",
    "                        best_mean_time = mean_time_taken_across_fold\n",
    "                        best_num_epochs = num_epochs\n",
    "        \n",
    "                    else:\n",
    "                        pass\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd78a3-13a2-4194-b132-9b8ca8eaf0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nBest validation size = {best_val_size}, Best train size = {best_train_size}, Best hidden dimension = {best_hidden_dim}, Best input dimension = {best_input_dim}, Best sequence length = {best_seq_len}, Best parameter count = {best_param_count}, Training adjusted sharpe = {best_mean_adj_sharpe_across_fold}, Validation adjusted sharpe = {best_mean_val_adj_sharpe_across_fold}, Best time = {best_mean_time}, Best number of epochs = {best_num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644f42d-6d58-4679-8e97-35c7fd7e98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best validation size = 8, Best train size = 2520, Best hidden dimension = 4, Best input dimension = 4, Best sequence length = 1, Best parameter count = 165, Training adjusted sharpe = 2.82, Validation adjusted sharpe = 3.376, Best time = 0.263\n",
    "\n",
    "#Validation size = 8, Train size = 1008, Hidden dim = 16, Input dimension: 16, Parameter count = 2193, Seq length = 1, 3.443\n",
    "#Validation size = 8, Train size = 1512, Hidden dim = 8, Input dimension: 4, Parameter count = 457, Seq length = 1, 3.401\n",
    "#Validation size = 8, Train size = 2016, Hidden dim = 4, Input dimension: 4, Parameter count = 165, Seq length = 1, 3.407\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455e325-ea8d-4025-b446-cc8b0b1b1d78",
   "metadata": {},
   "source": [
    "# Run TSCV with the best hyper-parameters with a higher number of folds\n",
    "This is done to obtain a more accurate measure of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e866222-e7f2-41d9-b148-9f3e0bea0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_size = 8\n",
    "best_step_size = best_val_size\n",
    "best_train_size = 5040\n",
    "best_hidden_dim = 8\n",
    "best_input_dim = 10\n",
    "best_seq_len = 1\n",
    "max_folds = 200\n",
    "retrain_freq_list = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f0769-2686-42af-8fc5-94dcff55a095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for retrain_freq in retrain_freq_list:\n",
    "    run_tscv(best_train_size, best_val_size, best_seq_len, best_step_size, best_input_dim, best_hidden_dim, comp_dataset_df, shuffle = False, max_folds = max_folds, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a67009-5781-4ec0-8dde-b6b6cf30ff29",
   "metadata": {},
   "source": [
    "# Attempting the best model on a Kaggle simulated environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ac44c-fd13-4c81-a2a6-a03cb112fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./kaggle/input/hull-tactical-market-prediction/train.csv', skip_blank_lines = True)\n",
    "lt_model = None\n",
    "pipeline = None\n",
    "retrain_df = None\n",
    "best_val_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020f1d1-968c-4631-a340-bdbb94423e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []\n",
    "pred_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285aabfd-dfae-4683-815f-59c3d80ad098",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "def predict(test: pl.DataFrame) -> float: #pl.DataFrame:\n",
    "    \n",
    "    global count, train_df, lt_model, retrain_df, pipeline\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_df = test.to_pandas().iloc[:, 0: -4] #To exclude some blank columns that are being read.\n",
    "    \n",
    "    #Copying 'forward_returns', 'risk_free_rate', and 'market_forward_excess_returns' from the current test row to the latests training row.\n",
    "    train_df.iloc[-1, [-3, -2, -1]] = test_df.iloc[0, [-3, -2, -1]]\n",
    "\n",
    "    #Retrain the model.\n",
    "    if count%best_val_size == 0:\n",
    "\n",
    "        #Prepare the training data.\n",
    "        retrain_df = train_df.iloc[-best_train_size:]\n",
    "        pipeline = Pipeline([('cleaner', CleanerTransformer()),\n",
    "                             ('latent', LatentTransformer(best_input_dim))])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                            \"ignore\",\n",
    "                            category=FutureWarning,\n",
    "                            module=\"sklearn.pipeline\"\n",
    "                        )\n",
    "            pipeline = pipeline.fit(retrain_df.iloc[:, :-3].values)\n",
    "            lt_train_latent_ten = torch.tensor(pipeline.transform(retrain_df.iloc[:, :-3].values), dtype=torch.float32)\n",
    "        \n",
    "        forward_returns_ten = torch.tensor(retrain_df['forward_returns'].values, dtype=torch.float32)\n",
    "        risk_free_rate_ten = torch.tensor(retrain_df['risk_free_rate'].values, dtype=torch.float32)\n",
    "\n",
    "        #We will then train the model here.\n",
    "        lt_model, param_count, adj_sharpe_best_epoch, best_val_adj_sharpe, time_taken, _ = train_model(lt_train_latent_ten, forward_returns_ten, risk_free_rate_ten, None, None, None, best_seq_len, best_input_dim, best_hidden_dim, 1, shuffle = False, num_epochs = 2, patience = 2, verbose = False)        \n",
    "\n",
    "    #Make the prediction\n",
    "    lt_val_latent_ten = torch.tensor(pipeline.transform(test_df.iloc[:, :-4].values), dtype=torch.float32)\n",
    "    \n",
    "    lt_pred = lt_model(lt_val_latent_ten.unsqueeze(0))\n",
    "        \n",
    "    #Append the current test row with blank 'forward_returns', 'risk_free_rate', and 'market_forward_excess_returns' to the training dataset.\n",
    "    cols = train_df.columns[:train_df.shape[1] - 3]\n",
    "    new_train_row_df = test_df.loc[:, cols]\n",
    "    train_df = pd.concat([train_df, new_train_row_df], axis = 0, ignore_index = True)\n",
    "\n",
    "    pred_time = round((time.time() - start_time)/60, 3)\n",
    "    print(f'Count = {count}, Predictions = {lt_pred.item()}, Prediction time = {pred_time}')\n",
    "    time_list.append(pred_time)\n",
    "\n",
    "    \"\"\"Replace this function with your inference code.\n",
    "    You can return either a Pandas or Polars dataframe, though Polars is recommended for performance.\n",
    "    Each batch of predictions (except the very first) must be returned within 5 minutes of the batch features being provided.\n",
    "    \"\"\"\n",
    "    pred_df = pl.DataFrame({\n",
    "        \"row_id\": test[\"date_id\"],      # must match exactly what's in test data\n",
    "        \"target\": pl.Series([lt_pred.item()] * test.height)  # dummy prediction\n",
    "    })\n",
    "\n",
    "    pred_list.append(lt_pred.item())\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57cc12-fa4c-42c9-9e76-17016713954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('./kaggle/input/hull-tactical-market-prediction/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a03c01-348e-4374-b281-710f4587829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean prediction time = {np.mean(time_list)}')\n",
    "print(f'Minimum prediction time = {np.min(time_list)}')\n",
    "print(f'Maximum prediction time = {np.max(time_list)}')\n",
    "print(\"\")\n",
    "print(f'Mean prediction = {np.mean(pred_list)}')\n",
    "print(f'Minimum prediction = {np.min(pred_list)}')\n",
    "print(f'Maximum prediction = {np.max(pred_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15952f3-47ac-4f1f-abfb-e8566c2218f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
